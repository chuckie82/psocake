# usage: mpirun -n 4 python diffusionMap.py amo87215 -s 64 -e 78 -t PR772 -o /reg/d/psdm/amo/amo87215/scratch/yoon82/data/ --verbose 0 --knn 500 --normalize --skipDist# usage: bsub -q psanaq -a mympi -n 15 -R "span[ptile=2]" -o %J.log python diffusionMap.py amo87215 -s 64 -e 78 -t PR772 -o /reg/d/psdm/amo/amo87215/scratch/yoon82/data/ --verbose 1 --knn 500 --outerR 80 --innerR 20 --normalize --skipDist# amo86615# bsub -q psanaq -a mympi -n 16 -R "span[ptile=2]" -o %J.log python diffusionMap.py amo86615 -s 191 -e 191 -t PR772 -o /reg/d/psdm/amo/amo86615/res/yoon82/data/ --verbose 0 --knn 100 --outerR 500 --innerR 100 --useADU --denseLaplacian# Reading of files should be done by the slaves# Better Friedel averaging needed# Check post-filtering based on particle sizeimport numpy as npimport matplotlib.pyplot as pltimport h5pyfrom sklearn.neighbors import NearestNeighborsfrom sklearn.metrics.pairwise import euclidean_distancesimport timefrom scipy import statsfrom scipy import specialimport matplotlib.pyplot as pltfrom matplotlib.path import Pathimport matplotlib.patches as patchesfrom scipy.sparse.linalg import svdsfrom scipy.sparse.linalg import eigs, eigshimport scipy.sparsefrom numpy.linalg import svdfrom numpy.matlib import repmatimport sysfrom psana import *from mpi4py import MPIimport psutilimport argparsefrom scipy import ndimagefrom scipy import miscimport osparser = argparse.ArgumentParser()parser.add_argument("expName", help="psana experiment (e.g. amo86615)")parser.add_argument("-r", "--runs", type=str)#parser.add_argument("-s","--startRun",help="number of events, all events=0", type=int)#parser.add_argument("-e","--endRun",help="append tag to end of filename", type=int)parser.add_argument("-t","--tag",help="tag", default=None, type=str)parser.add_argument("-ver","--version",help="version of the diffusion map",default=1, type=int)parser.add_argument("-d","--detectorName",help="psana detector alias, (e.g. pnccdBack, DsaCsPad)", type=str)#parser.add_argument("--outerR",help="outer radius of a donut mask (pixels)",default=2000, type=int)#parser.add_argument("--innerR",help="inner radius of a donut mask (pixels)",default=0, type=int)parser.add_argument("-v","--verbose",help="verbose",default=0, type=int)parser.add_argument("-c","--chunkSize",help="L2_distance chunk size per worker",default=5000, type=int)parser.add_argument("-k","--knn",help="k-nearest neighbors to keep for each data point",default=100, type=int)parser.add_argument("--numEigs",help="number of eigenvectors to calculate",default=3, type=int)parser.add_argument("-o","--outputPath",help="output path", type=str)parser.add_argument("--sigma",help="sigma for calculating the kernel",default=0, type=float)parser.add_argument("--normalize",help="variance normalize the input data, default=False", action='store_true')parser.add_argument("--skipDist",help="skip distance calculation",action='store_true')#parser.add_argument("--condition",help="comparator operation for choosing input data from an hdf5 dataset."#                                       "Must be comma separated for now."#                                       "Available comparators are: gt(>=), ge(>), eq(==), le(<=), lt(<)"#                                       "e.g. /particleSize/corrCoef,ge,0.85 ",default='', type=str)parser.add_argument("--denseLaplacian",help="dense laplacian",action='store_true')parser.add_argument("--useADU",help="use ADU images",action='store_true')parser.add_argument("--test",help="test mode where the images are replaced with test images",action='store_true')parser.add_argument("--sqrt",help="use square root of the images",action='store_true')parser.add_argument("--downsample",help="downsampling factor",default=1, type=int)parser.add_argument("--userMask",help="assembled user mask in npy", default=None, type=str)args = parser.parse_args()comm = MPI.COMM_WORLDrank = comm.Get_rank()size = comm.Get_size()assert size>=2, 'Require at least two mpi ranks'numSlaves = size-1status = MPI.Status()#if rank == 0:#    print "condition: ", args.conditionfilepath = args.outputPathexpName = args.expName#startRun = args.startRun#endRun = args.endRuntag = args.tagversion = args.versionverbose = args.verbose# Mask parameter#outerR = args.outerR#innerR = args.innerR# Distance calculationchunkSize = args.chunkSize# ManifoldsigmaK = args.sigmaalias = args.detectorName# hdf5 datasetgrpName = "/diffusionMap"dset_mask = '/mask'dset_normData = '/normData'dset_row = '/D_row'dset_col = '/D_col'dset_val = '/D_val'dset_data = '/D_data'dset_indices = '/D_indices'dset_indptr = '/D_indptr'dset_eigvec = '/eigvec'dset_eigval = '/eigval'dset_testData = '/testData'# Output filename#fname = filepath+'/'+expName+'_'+str(startRun)+'_'+str(endRun)+'_class_v'+str(version)+'.h5'fname = filepath + '/' + 'embed_' + expName + '.h5'def digestRunList(runList):    runsToDo = []    if not runList:        print "Run(s) is empty. Please type in the run number(s)."        return runsToDo    runLists = str(runList).split(",")    for list in runLists:        temp = list.split(":")        if len(temp) == 2:            for i in np.arange(int(temp[0]),int(temp[1])+1):                runsToDo.append(i)        elif len(temp) == 1:            runsToDo.append(int(temp[0]))    return runsToDodef varNorm(V):# variance normalization, each image has mean 0, variance 1    # This shouldn't happen, but zero out infinite pixels    V[np.argwhere(V==np.inf)] = 0    mean = np.mean(V)    std = np.std(V)    if std == 0:        return np.zeros_like(V)    V1 = (V-mean)/std    if np.isnan(np.sum(V1)) == True: embed()    assert np.isinf(np.sum(V1)) == False    return V1def fft2(image):    """    fft2 returns shifted fft of image    """    from scipy import fftpack    F1 = fftpack.fft2(image)    F1 = fftpack.fftshift( F1 )    return F1def powerSpectrum(image):    """    powerSpectrum returns the power spectrum of the image    """    F1 = fft2(image)    F1 = np.abs(F1)**2    return F1def averageFriedelPairs(img):    filledImg = np.zeros_like(img)    y = img.shape[0]    x = img.shape[1]    indY = np.linspace(0,y-1,y)    indX = np.linspace(0,x-1,x)    cy = img.shape[0]/2    cx = img.shape[1]/2    ind = np.argwhere(img==0)    for myIndY in indY:        for myIndX in indX:            dy = myIndY-cy            dx = myIndX-cx            if cy-dy >= 0 and cy-dy < img.shape[0] and cx-dx >= 0 and \               cy-dx < img.shape[1] and cy+dy >= 0 and cy+dy < img.shape[0] and \               cx+dx >= 0 and cy+dx < img.shape[1]:                if np.isnan(img[cy-dy,cx-dx])==0:                    if img[cy-dy,cx-dx] > 0:                        if img[cy+dy,cx+dx] > 0:                            filledImg[cy-dy,cx-dx] = (img[cy-dy,cx-dx] + img[cy+dy,cx+dx])/2                        else:                            filledImg[cy-dy,cx-dx] = img[cy-dy,cx-dx]                    else:                        if img[cy+dy,cx+dx] > 0:                            filledImg[cy-dy,cx-dx] = img[cy+dy,cx+dx]                        else:                            filledImg[cy-dy,cx-dx] = 0    return filledImgdef donutMask(N,M,R,r):    """    Calculate a donut mask.    N,M - The height and width of the image    R   - Maximum radius from center    r   - Minimum radius from center        """    centerY = N/2.    centerX = M/2.    mask = np.zeros((N,M))    for i in range(N):        xDist = i-centerY        xDistSq = xDist**2        for j in range(M):            yDist = j-centerX            yDistSq = yDist**2            rSq = xDistSq+yDistSq            if rSq <= R**2 and rSq >= r**2:                mask[i,j] = 1    return maskdef getUserMask(fname,dim0,dim1):    if fname is not None:        userMask = np.load(fname)    else:        userMask = np.ones((dim0,dim1))    return userMaskdef getMyUnfairShare(numJobs,numWorkers,rank):    """Returns number of events assigned to the slave calling this function."""    assert(numJobs >= numWorkers)    allJobs = np.arange(numJobs)    jobChunks = np.array_split(allJobs,numWorkers)    myChunk = jobChunks[rank]    myJobs = allJobs[myChunk[0]:myChunk[-1]+1]    return myJobsdef getMyFairShare(numJobs,numWorkers,rank):    """Returns number of events assigned to the slave calling this function."""    assert(numJobs >= numWorkers)    numJobs = int(numJobs/numWorkers)*numWorkers # This is the numJobs that can be performed in parallel    allJobs = np.arange(numJobs)    jobChunks = np.array_split(allJobs,numWorkers)    myChunk = jobChunks[rank]    myJobs = allJobs[myChunk[0]:myChunk[-1]+1]    return myJobsdef getRemainingShare(numJobs,numSlaves):    """Returns number of remaining events"""    assert(numJobs >= numSlaves)    numJobsDone = int(numJobs/numSlaves)*numSlaves # This is the numJobs that can be performed in parallel    myJobs = np.arange(numJobsDone,numJobs)    return myJobsdef matlabSort(A,axis=0,knn=0):    """    Returns sorted index and sorted value like Matlab sort    :param A: 2D numpy array    :param axis: sorting axis, 0 = along columns, 1 = along rows    :param knn: number of smallest elements to return    :return:        :param B: sorted value        :param I: sorted index    """    numRows,numCols = A.shape    if numRows == 1: # vector problem        I = np.argsort(A,axis=1) # sorted index        if knn > numCols or knn == 0:            knn = numCols        I = I[0,0:knn]        B = np.zeros((1,knn))        B = A[0,I]        return B, I    elif numRows > 1: # matrix problem        I = np.argsort(A,axis=axis) # sorted index        if axis == 0:            if knn > numRows or knn == 0:                knn = numRows            I = I[0:knn,:]            B = np.zeros((knn,numCols))            for j in range(A.shape[1]):                B[:,j] = A[I[:,j],j]        elif axis == 1:            if knn > numCols or knn == 0:                knn = numCols            I = I[:,0:knn]            B = np.zeros((numRows,knn))            for j in range(A.shape[0]):                B[j,:] = A[j,I[j,:]]        return B, Idef indexify(I,rowOffset):    import numpy.matlib    if len(I.shape) == 1:        knn = I.shape[0]        numRows = 1        r = np.arange(numRows) + rowOffset        r.shape = (numRows,1) # col vec        r = numpy.matlib.repmat(r,1,knn)    else:        numRows, knn = I.shape        r = np.arange(numRows) + rowOffset        r.shape = (numRows,1) # col vec        r = numpy.matlib.repmat(r,1,knn)    return rdef euclidean_distances_chunks(myHdf5,knn):    # Calculate chunk by chunk euclidean distances in parallel    ind = getMyFairShare(numHits,size,rank)    chunkSizeX = chunkSize    if chunkSizeX > numHits:        chunkSizeX = numHits    myChunksX = np.arange(0,numHits,chunkSizeX)    if myChunksX.size != 0:        if myChunksX[-1] < numHits:            myChunksX = np.append(myChunksX,numHits)    myChunksY = np.array([ind[0],ind[-1]+1])    # Calculate number of element    knnSize = knn    if knn > chunkSizeX:        knnSize = chunkSizeX    leftOver = np.mod(numHits,chunkSizeX)    if knn < leftOver:        leftOver = knn    expectedSize = int(numHits/chunkSizeX)*knnSize + leftOver    myUnsortedD = np.zeros((ind.size,numHits))    myD = np.zeros((ind.size,expectedSize))    myInd = np.zeros_like(myD,dtype='uint32')    yStart = myChunksY[0]    yEnd = myChunksY[1]    a = myHdf5[grpName+dset_normData][yStart:yEnd,:]    currentIndX = 0    for j in range(myChunksX.size-1):        xStart = myChunksX[j]        xEnd = myChunksX[j+1]        numX = xEnd-xStart        _knn = knn        if _knn > numX:            _knn = numX        b = myHdf5[grpName+dset_normData][xStart:xEnd,:]        D = euclidean_distances(a,b)        print "*** done euclidean distance"        sortedD, sortedI = matlabSort(D,axis=1,knn=_knn)        sortedI += xStart # offset by global chunk position in x        myUnsortedD[:,xStart:xEnd] = D        myD[:,currentIndX:currentIndX+_knn] = sortedD        myInd[:,currentIndX:currentIndX+_knn] = sortedI        currentIndX += _knn    # At least knn values per row, only save the smallest knn values    val, col= matlabSort(myD,axis=1,knn=knn)    globalI = np.zeros_like(col)    for j in range(col.shape[0]):        globalI[j,:] = myInd[j,col[j,:]]    row = indexify(globalI,myChunksY[0]) # offset by global chunk position in y    myHdf5[grpName+dset_row][ind[0]:ind[-1]+1,:] = row    myHdf5[grpName+dset_col][ind[0]:ind[-1]+1,:] = globalI #col    myHdf5[grpName+dset_val][ind[0]:ind[-1]+1,:] = valdef euclidean_distances_line(myHdf5,knn):    # Do line by line euclidean distance in parallel    ind = getMyFairShare(numHits,size,rank)    numDone = ind.size*size    if numDone < numHits: # some work to be done        myShare = np.arange(rank,numHits,size)        notDoneInd = np.where(myShare>numDone-1)        todoInd = myShare[notDoneInd]        chunkSizeX = chunkSize        if chunkSizeX > numHits:            chunkSizeX = numHits        # Calculate number of element        knnSize = knn        if knn > chunkSizeX:            knnSize = chunkSizeX        leftOver = np.mod(numHits,chunkSizeX)        if knn < leftOver:            leftOver = knn        expectedSize = int(numHits/chunkSizeX)*knnSize + leftOver        for i in todoInd:            myD = np.zeros((1,expectedSize))            myInd = np.zeros_like(myD,dtype='uint32')            currentIndX = 0            myChunksX = np.arange(0,numHits,chunkSizeX)            if myChunksX.size != 0:                if myChunksX[-1] < numHits:                    myChunksX = np.append(myChunksX,numHits)                for j in range(myChunksX.size-1):                    xStart = myChunksX[j]                    xEnd = myChunksX[j+1]                    numX = xEnd-xStart                    _knn = knn                    if _knn > numX:                        _knn = numX                    a = myHdf5[grpName+dset_normData][i,:]                    b = myHdf5[grpName+dset_normData][xStart:xEnd,:]                    D = euclidean_distances(a,b)                    sortedD, sortedI = matlabSort(D,axis=1,knn=_knn)                    sortedI += xStart # offset by global chunk position in x                    myD[0,currentIndX:currentIndX+_knn] = sortedD                    myInd[0,currentIndX:currentIndX+_knn] = sortedI                    currentIndX += _knn            # At least knn values per row, only save the smallest knn values            val, col = matlabSort(myD,axis=1,knn=knn)            globalI = np.zeros_like(col)            globalI[0:knn] = myInd[0,col[0:knn]]            row = indexify(globalI,i)            myHdf5[grpName+dset_row][i,:] = row            myHdf5[grpName+dset_col][i,:] = globalI #col            myHdf5[grpName+dset_val][i,:] = valdef euclidean_distances_sym(myHdf5,knn):    row = np.array(myHdf5[grpName+dset_row])    col = np.array(myHdf5[grpName+dset_col])    val = np.array(myHdf5[grpName+dset_val])    import scipy.sparse    import scipy    D = scipy.sparse.csc_matrix((val.flatten(),(row.flatten(),col.flatten())), shape=(numHits,numHits))    #embed()    D.setdiag(1e-10) # set diagonal to zero    tic = time.time()    #D = scipy.sparse.triu(D)    #D += D.transpose()    D = np.maximum(D.todense(),D.todense().transpose())    D = scipy.sparse.csc_matrix(D)    toc = time.time()    print "time1: ", toc-tic    #tic = time.time()    if 0:        import scipy.io        D25 = scipy.io.loadmat('/reg/neh/home/pschwan/DM_Test_Public_20160330/test1/dataY/dataY_nS10000_nN25_iB1.mat')        yVal = D25['yVal']        yVal = np.sqrt(yVal)        yInd = D25['yInd']-1        D = scipy.sparse.csc_matrix((yVal.flatten(),(row.flatten(),yInd.flatten())), shape=(numHits,numHits))        D = np.maximum(D.todense(),D.todense().transpose())        D = scipy.sparse.csc_matrix(D)        #toc = time.time()        #print "time2: ", toc-tic    if grpName+dset_data in myHdf5:        del myHdf5[grpName+dset_data]        del myHdf5[grpName+dset_indices]        del myHdf5[grpName+dset_indptr]        del myHdf5[grpName+dset_row]        del myHdf5[grpName+dset_col]        del myHdf5[grpName+dset_val]    dset = myHdf5.create_dataset(grpName+dset_data, (D.data.shape), dtype='float64')    dset[...] = D.data    dset = myHdf5.create_dataset(grpName+dset_indices, (D.indices.shape), dtype='uint32')    dset[...] = D.indices    dset = myHdf5.create_dataset(grpName+dset_indptr, (D.indptr.shape), dtype='uint32')    dset[...] = D.indptr    myHdf5[grpName+dset_data].attrs['numHits'] = numHits    if args.verbose >= 2:        D_dense = D.todense()        plt.imshow(D_dense,interpolation='none')        plt.colorbar()        plt.title('final distance matrix')        plt.show()        # Show knn        imgInd = 160        knnInd = np.where(np.asarray(D_dense[imgInd,:]>0))[1]        value = np.squeeze(np.asarray(D_dense[imgInd,knnInd]))        sortInd = np.argsort(value)        print "imgInd, knnInd: ", imgInd, knnInd, value, sortInd        plt.subplot(331)        plt.plot(myHdf5[grpName+dset_normData][imgInd,:],'rx')        plt.subplot(332)        plt.plot(myHdf5[grpName+dset_normData][knnInd[sortInd[0]],:],'x')        plt.title(value[sortInd[0]])        plt.subplot(333)        plt.plot(myHdf5[grpName+dset_normData][knnInd[sortInd[1]],:],'x')        plt.title(value[sortInd[1]])        plt.subplot(334)        plt.plot(myHdf5[grpName+dset_normData][knnInd[sortInd[2]],:],'x')        plt.title(value[sortInd[2]])        plt.subplot(335)        plt.plot(myHdf5[grpName+dset_normData][knnInd[sortInd[3]],:],'x')        plt.title(value[sortInd[3]])        plt.subplot(336)        plt.plot(myHdf5[grpName+dset_normData][knnInd[sortInd[4]],:],'x')        plt.title(value[sortInd[4]])        plt.subplot(337)        plt.plot(myHdf5[grpName+dset_normData][knnInd[sortInd[5]],:],'x')        plt.title(value[sortInd[5]])        plt.subplot(338)        plt.plot(myHdf5[grpName+dset_normData][knnInd[sortInd[6]],:],'x')        plt.title(value[sortInd[6]])        plt.subplot(339)        plt.plot(myHdf5[grpName+dset_normData][knnInd[sortInd[7]],:],'x')        plt.title(value[sortInd[7]])        plt.show()def get_euclidean_distances(fname, numHits):    # Everyone calculate distance    myHdf5 = h5py.File(fname, 'r+', driver='mpio', comm=comm)    grp = myHdf5.require_group(grpName)    if grpName+dset_row in myHdf5:        del myHdf5[grpName+dset_row]        del myHdf5[grpName+dset_col]        del myHdf5[grpName+dset_val]    knn = args.knn    if args.knn > numHits:        knn = numHits    myHdf5.create_dataset(grpName+dset_row, (numHits,knn), dtype='uint32')    myHdf5.create_dataset(grpName+dset_col, (numHits,knn), dtype='uint32')    myHdf5.create_dataset(grpName+dset_val, (numHits,knn), dtype='float64')    euclidean_distances_chunks(myHdf5,knn)    myHdf5.close()    comm.Barrier()    if args.verbose >= 2 and rank == 0:        myHdf5 = h5py.File(fname, 'r')        plt.subplot(131)        plt.imshow(myHdf5[grpName+dset_row],interpolation='none')        plt.title('check knn row')        plt.subplot(132)        plt.imshow(myHdf5[grpName+dset_col],interpolation='none')        plt.title('check knn col')        plt.subplot(133)        plt.imshow(myHdf5[grpName+dset_val],interpolation='none')        plt.title('check knn val')        plt.show()        myHdf5.close()    comm.Barrier()    # fill in the remaining L2_norm    myHdf5 = h5py.File(fname, 'r+', driver='mpio', comm=comm)    euclidean_distances_line(myHdf5,knn)    myHdf5.close()    comm.Barrier()    if args.verbose >= 2 and rank == 0:        myHdf5 = h5py.File(fname, 'r')        plt.subplot(131)        plt.imshow(myHdf5[grpName+dset_row],interpolation='none')        plt.title('check knn row')        plt.subplot(132)        plt.imshow(myHdf5[grpName+dset_col],interpolation='none')        plt.title('check knn col')        plt.subplot(133)        plt.imshow(myHdf5[grpName+dset_val],interpolation='none')        plt.title('check knn val')        plt.show()        myHdf5.close()    comm.Barrier()    # Symmetrize    if rank == 0:        print "symmetrizing euclidean distance"        myHdf5 = h5py.File(fname, 'r+')        euclidean_distances_sym(myHdf5,knn)        myHdf5.close()    comm.Barrier()def updateState():    if rank == 0:       state = {'filenameList': filenameList,                'hitInd': allHitInd,                'mask': mask}    else:       state = None    state = comm.bcast(state, root=0)    return statedef updateStateSigma():    if rank == 0:       state = {'sigma': sigmaK}    else:       state = None    state = comm.bcast(state, root=0)    return statedef loadDataIn(filenameList,allHitInd,maskInd,myHdf5,single=False,loadFile=False):    numHits = len(allHitInd)    # Send each slave a list of images to load and downsample and variance normalize    if single:        ind = getRemainingShare(numHits,size)    else:        ind = getMyFairShare(numHits,size,rank)    fileList = filenameList[ind[0]:ind[-1]+1]    hitIndList = allHitInd[ind[0]:ind[-1]+1]    if verbose >= 1: print "### instructions: ", rank, fileList[0], fileList[-1], len(fileList)    #print "@#$@#$@##@ ", rank, myHdf5[grpName].keys(), myHdf5[grpName+dset_normData], myHdf5[grpName+dset_normData][0,:]    #print "POPOPOPOPOP"    if loadFile:        isFirstTime = True        currentFileOpen = ''        for i, fname in enumerate(fileList):            if os.path.exists(fname):                if isFirstTime:                    # Load files                    f = h5py.File(fname,'r')                    if verbose >= 1: print "### file open: ", rank, fname                    if args.useADU:                        adu = f['/photonConverter/'+alias+'/adu'].value                    else:                        photons = f['/photonConverter/'+alias+'/photonCount'].value                    dim_row = f['/photonConverter/'+alias+'/photonCount'].attrs['downsampleRows']                    dim_col = f['/photonConverter/'+alias+'/photonCount'].attrs['downsampleCols']                    currentFileOpen = fname                    isFirstTime = False                elif fname not in currentFileOpen:                    f.close()                    f = h5py.File(fname,'r')                    if verbose >= 1: print "### file open: ", rank, fname                    if args.useADU:                        adu = f['/photonConverter/'+alias+'/adu'].value                    else:                        photons = f['/photonConverter/'+alias+'/photonCount'].value                    dim_row = f['/photonConverter/'+alias+'/photonCount'].attrs['downsampleRows']                    dim_col = f['/photonConverter/'+alias+'/photonCount'].attrs['downsampleCols']                    currentFileOpen = fname                # load image as 2D                localInd = hitIndList[i]                if args.useADU:                    myPhot=adu[localInd,:,:]                else:                    if len(photons.shape) == 2: # each event is 1-d array                        myPhot=photons[localInd,:]                    elif len(photons.shape) == 3: # each event is 2-d array                        myPhot=photons[localInd,:,:]                if args.sqrt:                    myPhot /= 1.0*np.sum(myPhot)                    myPhot = np.sqrt(abs(myPhot))                globalInd = ind[0] + i                # Hijack here to put fake images for testing                if args.test:                    if (0):                        im1 = misc.face(gray=True)                        crow =  im1.shape[0]/2                        ccol = im1.shape[1]/2                        rotationIncrement = 360./len(allHitInd) # degrees                        myPhot = ndimage.rotate(im1,globalInd*rotationIncrement,reshape=False)[crow-np.floor(dim_row/2.):crow+np.ceil(dim_row/2.),ccol-np.floor(dim_col/2.):ccol+np.ceil(dim_col/2.)]*mask                        myHdf5[grpName+dset_testData][globalInd,:,:] = myPhot                # variance normalization                if args.normalize:                    myNormData = varNorm(myPhot.flatten()[np.squeeze(maskInd)])                else:                    myNormData = myPhot.flatten()[np.squeeze(maskInd)]                myHdf5[grpName+dset_normData][globalInd,:] = myNormData    else:        # Load images from xtc        currentFileOpen = ''        for i, fname in enumerate(fileList):            localInd = hitIndList[i]            # Read in hit events            if os.path.exists(fname) and currentFileOpen is not fname:                # setup                name = fname.split('/')[-1].split('.cxi')[0]                if len(name.split('_')) == 2:                    (experimentName, runNumber) = name.split('_')                elif len(name.split('_')) == 3:                    (experimentName, runNumber, cxi_tag) = name.split('_')                else:                    print "Invalid filename: ", fname                #experimentName = fname.split('_')[0].split('/')[-1]                #runNumber = fname.split('_')[1].split('.cxi')[0]                #print "##### experimentName, runNumber: ", experimentName,runNumber,alias                run,times,det,evt = setup(experimentName,runNumber,alias)                # Load files                f = h5py.File(fname,'r')                if verbose >= 1: print "### file open: ", rank, fname                #if args.useADU:                hitEvent = f['/LCLS/eventNumber'].value                f.close()                currentFileOpen = fname            # Get hit image            if args.useADU:                evt = run.event(times[hitEvent[localInd]])                myPhot = det.image(evt)                myPhot = sm.block_reduce(myPhot,block_size=(args.downsample,args.downsample),func=np.sum)            else:                evt = run.event(times[hitEvent[localInd]])                #print "#### hitIndList: ", localInd, len(hitEvent), len(hitIndList),experimentName,runNumber,alias                myPhot = det.image(evt)                myPhot = sm.block_reduce(myPhot,block_size=(args.downsample,args.downsample),func=np.sum)                # TODO: Convert to photons            if args.sqrt:                myPhot /= 1.0*np.sum(myPhot)                myPhot = np.sqrt(abs(myPhot))            globalInd = ind[0] + i            # Hijack here to put fake images for testing            if args.test:                if (0):                    im1 = misc.face(gray=True)                    crow =  im1.shape[0]/2                    ccol = im1.shape[1]/2                    rotationIncrement = 360./len(allHitInd) # degrees                    myPhot = ndimage.rotate(im1,globalInd*rotationIncrement,reshape=False)[crow-np.floor(dim_row/2.):crow+np.ceil(dim_row/2.),ccol-np.floor(dim_col/2.):ccol+np.ceil(dim_col/2.)]*mask                    myHdf5[grpName+dset_testData][globalInd,:,:] = myPhot            # variance normalization            if args.normalize:                myNormData = varNorm(myPhot.flatten()[np.squeeze(maskInd)])            else:                myNormData = myPhot.flatten()[np.squeeze(maskInd)]            #print "#### globalInd: ", globalInd, myNormData.shape, grpName+dset_normData            myHdf5[grpName+dset_normData][globalInd,:] = myNormData    if verbose >= 3:        plt.subplot(121)        plt.imshow(np.log10(myPhot*mask),interpolation='none')        plt.title(str(rank)+"_"+str(i))        plt.subplot(122)        plt.plot(myNormData,'x')        plt.title(str(rank)+"_"+str(i))        plt.show()def loadData(filenameList,allHitInd,maskInd):    if rank == 0: tic = time.time()    # Parallel load    if verbose >= 1: print "Loading in images by chunks"    print "### loadData fname: ", fname    myHdf5 = h5py.File(fname, 'r+', driver='mpio', comm=comm)    print "### WHAT: ", rank, grpName+dset_normData, myHdf5, myHdf5.keys(), myHdf5[grpName].keys()    loadDataIn(filenameList,allHitInd,maskInd,myHdf5,single=False)    myHdf5.close()    # Load in remaining images, if any    if rank == 0:        numHitsPerWorker = len(getMyFairShare(numHits,size,0))        numDone = numHitsPerWorker*size        if verbose >= 1: print "Loading in remaining images ", numHits-numDone        if numDone < numHits:            # Write the remainder            myHdf5 = h5py.File(fname, 'r+')            loadDataIn(filenameList,allHitInd,maskInd,myHdf5,single=True)            myHdf5.close()        toc = time.time()        print "Done loading data", toc-ticdef setup(experimentName,runNumber,detInfo):    ds = DataSource('exp='+str(experimentName)+':run='+str(runNumber)+':idx')    run = ds.runs().next()    times = run.times()    env = ds.env()    evt = run.event(times[0])    det = Detector(str(detInfo), env)    return run,times,det,evtdef generateTodoList():    # Get number of hits    myRuns = digestRunList(args.runs)#np.arange(startRun,endRun+1)    hitsPerRun = []    allHitInd = np.array([])    filenameList = []    for i in myRuns:        print "tag: ", tag        if tag is not None:            print "tag provided"            fname = filepath+'/r'+str(i).zfill(4)+'/'+expName+'_'+str(i).zfill(4)+'_'+tag+'.cxi'        else:            print "tag not provided"            fname = filepath+'/r'+str(i).zfill(4)+'/'+expName+'_'+str(i).zfill(4)+'.cxi'                    print "fname: ", fname        if os.path.isfile(fname):            f = h5py.File(fname,'r')            numHits = f['/entry_1/result_1/nHits'].attrs['numEvents']            hitInd = np.arange(0,numHits)            if numHits > 0:                hitsPerRun.append(numHits)                allHitInd = np.append(allHitInd,hitInd)                filenameList.extend([fname for i in range(numHits)])            f.close()    totalNumHits = np.sum(hitsPerRun)    allHitInd = allHitInd.astype(int)    if verbose >= 1:        print "total numHits: ", hitsPerRun, totalNumHits        print "filenameList: ", filenameList[0], filenameList[-1], len(filenameList)        print "allHitInd: ", allHitInd.shape, allHitInd        print "numSlaves: ", numSlaves    return filenameList,allHitInddef setupHdf5():    print "Saving results to: ", fname    myHdf5 = h5py.File(fname, 'a')    if grpName in myHdf5:        print "Deleting grpName"        del myHdf5[grpName]    grp = myHdf5.create_group(grpName)    numPixelsMask = len(maskInd)    print "numHits,numPixelsMask: ", numHits,numPixelsMask    myHdf5.create_dataset(grpName+dset_normData, (numHits,numPixelsMask), dtype='float64', chunks=(1,numPixelsMask))    print "$$$ setupHdf5: ", myHdf5.keys(), myHdf5[grpName+dset_normData], myHdf5[grpName+dset_normData][0,0]    if args.test:        myHdf5.create_dataset(grpName+dset_testData, (numHits,mask.shape[0],mask.shape[1]), dtype='float64', chunks=(1,mask.shape[0],mask.shape[1]))    myHdf5.close()def diffusionMap(sigmaK, alpha=1, numEigs=16):    myHdf5 = h5py.File(fname, 'r+')    D_data = myHdf5[grpName+dset_data].value    numHits = myHdf5[grpName+dset_data].attrs['numHits']    D_indices = myHdf5[grpName+dset_indices]    D_indptr = myHdf5[grpName+dset_indptr]    if args.denseLaplacian: # dense laplacian        print "dense laplacian: ", sigmaK, alpha        #D = scipy.sparse.csc_matrix((D_data,D_indices,D_indptr),shape=(numHits,numHits)) #.tocoo()        K = np.exp(-np.square(D_data/sigmaK)) # kernel        K = scipy.sparse.csc_matrix((K,D_indices,D_indptr),shape=(numHits,numHits)).todense()        p = np.matrix(np.sum(K,axis=0))        P = np.multiply(p.transpose(),p)        if alpha == 1:            K1 = K/P        else:            K1 = K/np.power(P,alpha)        v = np.sqrt(np.sum(K1,axis=0))        A = K1/np.multiply(v.transpose(),v)        #if sigmaK >= 0.5:        #    thresh = 1e-7        #    M = np.max(A)        #    A[A<thresh*M]=0        #    u,s,vt = svds(scipy.sparse.coo_matrix(A),k=numEigs+1,which='LM')        #    u = np.fliplr(u)        #    U = u/repmat(np.matrix(u[:,0]).transpose(),1,numEigs+1)        #else:        #u,s,v = svd(A)        if numEigs+1 > numHits: numEigs = numHits - 2         s,u = eigsh(scipy.sparse.coo_matrix(A), k=numEigs+1, which='LM')        u = np.real(u)        u = np.fliplr(u)        s = s[::-1]        U = u/repmat(np.matrix(u[:,0]).transpose(),1,numEigs+1)        Y = U[:,1:numEigs+1]    else: # sparse laplacian        print "sparse laplacian"        yVal = np.exp(-np.square(D_data/sigmaK))        D2 = scipy.sparse.csc_matrix((yVal,D_indices,D_indptr),shape=(numHits,numHits)).tocoo()        if alpha == 1:            d = D2.sum(axis=0)        else:            d = np.power(D2.sum(axis=0),alpha)        d = np.transpose(d)        denom = np.squeeze(np.asarray(d[D2.row]))        denom1 = np.squeeze(np.asarray(d[D2.col]))        yVal = yVal / denom / denom1        # normalized laplacian        D4 = scipy.sparse.csc_matrix((yVal,D_indices,D_indptr),shape=(numHits,numHits)).tocoo()        d = np.power(D4.sum(axis=0),0.5)        d = np.transpose(d)        denom = np.squeeze(np.asarray(d[D4.row]))        denom1 = np.squeeze(np.asarray(d[D4.col]))        print "$#@%@$#@: ", D4.shape, denom.shape, denom1.shape        yVal = yVal / denom / denom1        # numerical stability        # l = abs(l+l.transpose())/2        tic = time.time()        print "Calculating eigenvectors"        if numEigs+1 > numHits: numEigs = numHits-2        s, u = eigsh( scipy.sparse.csc_matrix((yVal,D_indices,D_indptr),shape=(numHits,numHits)).tocoo(),k=numEigs+1,which='LM')        s = s[::-1]        u = np.real(u)        u = np.fliplr(u)        toc = time.time()        print "%%%%: ",toc-tic, u, u.shape, s, s.shape        U = u/repmat(np.matrix(u[:,0]).transpose(),1,numEigs+1)        Y = U[:,1:numEigs+1]    return Y,sdef getFileIndex(globalIndex,accumHits,numHitsPerFile):    for i, val in reversed(list(enumerate(accumHits))):        print "i,val: ", i, val        if globalIndex < val and globalIndex >= val-numHitsPerFile[i]:            localInd = globalIndex-(val-numHitsPerFile[i])            if i == 0:                skip = 0            else:                skip = accumHits[i-1]            print "### global, fileInd, local", globalIndex, i, localInd, skip            return i, int(allHitInd[localInd+skip])def showKNN(imgInd):    knnInd = np.where(np.asarray(D_dense[imgInd,:]>0))[1]    value = np.squeeze(np.asarray(D_dense[imgInd,knnInd]))    sortInd = np.argsort(value)    print "imgInd, knnInd: ", imgInd, knnInd, value, sortInd    allHitInd = np.array([])    numHitsPerFile = []    numFiles = 0    for r in digestRunList(args.runs):#np.arange(startRun,endRun+1):        if tag is not None:            filename = filepath+'/'+expName+'_'+str(r).zfill(4)+'_'+tag+'.cxi'        else:       	    filename = filepath+'/'+expName+'_'+str(r).zfill(4)+'.cxi'        if os.path.exists(filename):            print "filename :", filename            f1 = h5py.File(filename,'r')            # Condition:            if 0:                corrCoef = f1['/particleSize/corrCoef'].value                #print "corrCoef: ", corrCoef                hitInd = np.argwhere(corrCoef>=0.85)                numHits = len(hitInd)            else:                numHits = f1['/entry_1/result_1/nHits'].attrs['numEvents']                hitInd = np.arange(0,numHits)            allHitInd = np.append(allHitInd,hitInd)            if numHits > 0:                numHitsPerFile.append(numHits)                numFiles += 1            f1.close()    accumHits = np.zeros(numFiles,)    for i,val in enumerate(numHitsPerFile):        accumHits[i] = np.sum(numHitsPerFile[0:i+1])    print "accumHits, numHitsPerFile: ", accumHits, numHitsPerFile    # Get reference image    f = h5py.File(filenameList[imgInd],'r')    if args.useADU:        adu = f['/entry_1/instrument_1/detector_1/data'].value        dim_row = adu.shape[0]        dim_col = adu.shape[1]    else:        photons = f['/entry_1/instrument_1/detector_1/photons'].value        dim_row = photons.shape[0]        dim_col = photons.shape[1]    localInd = getFileIndex(imgInd,accumHits,numHitsPerFile)[1]    print "imgInd, localInd: ",imgInd,localInd    if args.useADU:        #myPhot=adu[localInd].reshape(dim_row,dim_col).astype(np.float64)        myPhot=adu[localInd,:,:]    else:        myPhot = photons[localInd,:,:]    f.close()    if args.test:        f1 = h5py.File(fname,'r')        myPhot = f1[grpName+'/testData'][imgInd,:,:]        f1.close()    myRefImage = myPhot    myImages = np.zeros((dim_row,dim_col,len(knnInd)))    for i in range(len(knnInd)):        # load image as 2D        globalInd = knnInd[sortInd[i]]        localInd = getFileIndex(allHitInd[globalInd],accumHits,numHitsPerFile)[1]        print "globalInd, localInd: ",globalInd,localInd        f = h5py.File(filenameList[globalInd],'r')        if args.useADU:            adu = f['/entry_1/instrument_1/detector_1/data'].value        else:            photons = f['/entry_1/instrument_1/detector_1/photons'].value            #index = f['/photonConverter/'+alias+'/index'].value        if args.useADU:            #myPhot=adu[localInd].reshape(dim_row,dim_col).astype(np.float64)            myPhot = adu[localInd,:,:]        else:            myPhot = photons[localInd,:,:]        f.close()        if args.test:            f1 = h5py.File(fname,'r')            myPhot = f1[grpName+'/testData'][globalInd,:,:]            f1.close()        myImages[:,:,i] = myPhot    plt.subplot(331)    plt.imshow(np.log10(myRefImage*mask),interpolation='none')    plt.subplot(332)    plt.imshow(np.log10(myImages[:,:,0]*mask),interpolation='none')    plt.title(value[sortInd[0]])    plt.subplot(333)    plt.imshow(np.log10(myImages[:,:,1]*mask),interpolation='none')    plt.title(value[sortInd[1]])    plt.subplot(334)    plt.imshow(np.log10(myImages[:,:,2]*mask),interpolation='none')    plt.title(value[sortInd[2]])    plt.subplot(335)    plt.imshow(np.log10(myImages[:,:,3]*mask),interpolation='none')    plt.title(value[sortInd[3]])    plt.subplot(336)    plt.imshow(np.log10(myImages[:,:,4]*mask),interpolation='none')    plt.title(value[sortInd[4]])    plt.subplot(337)    plt.imshow(np.log10(myImages[:,:,5]*mask),interpolation='none')    plt.title(value[sortInd[5]])    plt.subplot(338)    plt.imshow(np.log10(myImages[:,:,6]*mask),interpolation='none')    plt.title(value[sortInd[6]])    plt.subplot(339)    plt.imshow(np.log10(myImages[:,:,7]*mask),interpolation='none')    plt.title(value[sortInd[7]])    plt.show()#################################### Start from here! ####################################startRun = digestRunList(args.runs)[0]# Setup experiment_,_,det,evt = setup(expName,startRun,alias)import skimage.measure as smimg = det.image(evt)downImg = sm.block_reduce(img,block_size=(args.downsample,args.downsample),func=np.sum)(dim0,dim1) = downImg.shape# TODO: Generate maskuserMask = det.image(evt, getUserMask(args.userMask,dim0,dim1))downsampleMask = sm.block_reduce(userMask,block_size=(args.downsample,args.downsample),func=np.sum)mask = np.ones_like(downsampleMask,dtype=int)mask[np.where(downsampleMask==0)]=0#mask = donutMask(dim0,dim1,args.outerR,args.innerR) * mask#plt.imshow(mask)#plt.show()if args.skipDist is False:    # Generate to-do list    if rank == 0:        print "Generating todo list"        filenameList,allHitInd = generateTodoList()    comm.Barrier()    # All workers get the to-do list    state = updateState()    filenameList = state.get('filenameList')    allHitInd = state.get('hitInd')    mask = state.get('mask')    #print "mask!: ", mask.shape    # Generate donut mask and user-defined mask    #donutMask(dim0,dim1,args.outerR,args.innerR) #*    print "mask: ", mask    numHits = len(allHitInd)    reshapeMask = np.reshape(mask,(1,-1))    maskInd = np.argwhere(np.squeeze(reshapeMask)==1)    print "maskInd: ", maskInd.shape    comm.Barrier()    # Create hdf5 datasets    if rank == 0:        setupHdf5()    comm.Barrier()    # Parallel load in ADUs or Photons, optionally variance normalize, temporarily save as normData in hdf5    loadData(filenameList,allHitInd,maskInd)    comm.Barrier()    # Calculate euclidean distances    tic = time.time()    get_euclidean_distances(fname, numHits)    toc = time.time()    if rank == 0: print "Distance calculation time: ", toc-tic    # Check correct neighbors are saved    if rank == 0 and args.verbose >= 2:        myHdf5 = h5py.File(fname, 'r')        D_data = myHdf5[grpName+dset_data].value        numHits = myHdf5[grpName+dset_data].attrs['numHits']        D_indices = myHdf5[grpName+dset_indices]        D_indptr = myHdf5[grpName+dset_indptr]        D = scipy.sparse.csc_matrix((D_data,D_indices,D_indptr),shape=(numHits,numHits)).tocoo()        D_dense = D.todense()        # Show knn        showKNN(0)        myHdf5.close()    comm.Barrier()if args.sigma == 0 and rank == 0:    print "Delete L group"    myHdf5 = h5py.File(fname, 'r+')    if grpName+'/L' in myHdf5: del myHdf5[grpName+'/L']    myHdf5.close()print "Got here: ", rankcomm.Barrier()######################################## Calculate optimum sigma if not given#######################################if args.sigma == 0:    print "Calculate optimum sigma"    logEps = np.linspace(-10.0, 20.0, num=20)    eps = np.exp(logEps)    #assert(numSlaves+1 >= len(eps))    logSampleDistance = logEps[1]-logEps[0] # modify this if eps changes    logEps = np.log(eps)    print "fname: ", fname    myHdf5 = h5py.File(fname, 'r+', driver='mpio', comm=comm)    if args.skipDist:        numHits = myHdf5[grpName+dset_data].attrs['numHits'] # FIXME    print "numHits: ", numHits    D_data = myHdf5[grpName+dset_data].value    D_indices = myHdf5[grpName+dset_indices]    D_indptr = myHdf5[grpName+dset_indptr]    D = scipy.sparse.csc_matrix((D_data,D_indices,D_indptr),shape=(numHits,numHits))    dset = myHdf5.create_dataset(grpName+'/L', (len(eps),))    if rank == 0:        for i,e in enumerate(eps):            K = np.exp( -np.square(D_data)/ (2*e) ) # kernel            dset[i] = np.log(np.sum(K)+1e-10)    if 0:        if rank < len(eps):            tic = time.time()            K = np.exp( -np.square(D_data)/ (2*eps[rank]) ) # kernel            dset[rank] = np.log(np.sum(K))            toc = time.time()            print "times: ", rank, toc-tic            print "Rank "+str(rank)+" trying sigma "+str(eps[rank])+" resulted in "+str(dset[rank])    myHdf5.close()comm.Barrier()if args.sigma == 0 and rank == 0:    myHdf5 = h5py.File(fname, 'r')    L = myHdf5[grpName+'/L'].value    # Normalize the curve    normL = L - np.min(L) # min is 0    normL /= np.max(normL) # max is 1    print "$$$ L curve: ", L    print "$$$ normL curve: ", normL    # Linear fit    gradients=np.gradient(normL)    maxInd=np.argmax(gradients)    if maxInd < len(gradients)-1:        gradient = (normL[maxInd+1]-normL[maxInd]) / logSampleDistance        # Calculate when it will reach 0.75        targetY = 0.75        # deltaX = deltaY/gradient + startingX        sigmaK = np.exp((targetY - normL[maxInd]) / gradient + logEps[maxInd]) # gradient is in log space        print "$$$ optimum sigmaK: ", sigmaK        if verbose >= 2:            myFit = np.zeros_like(normL)            myFit[maxInd] = normL[maxInd]            myFit[maxInd+1] = normL[maxInd]+(gradient*log10sampleDistance)            print "myFit: ", myFit            print "optimization: ", logEps, normL            plt.plot(logEps,normL,'x-')            plt.plot(logEps,myFit,'ro-')            plt.title('sigma curve')            plt.show()    else:        print "Warning: Increase sigmaK"        sigmaK = np.exp(logEps[maxInd])    myHdf5.close()comm.Barrier()state = updateStateSigma()sigmaK = state.get('sigma')#print "sigmaK: ", rank, sigmaKif rank == 0:    # Manifold embedding    print "run diffusion map"    Y,s = diffusionMap(sigmaK,numEigs=args.numEigs)    numHits = Y.shape[0]    print "Y: ", Y.shape, s.shape    ################################    # Save manifold in hdf5 and exit    print "Saving to: ", fname    myHdf5 = h5py.File(fname, 'r+')    grp = myHdf5.require_group(grpName)    if grpName+dset_eigvec in myHdf5:        del myHdf5[grpName+dset_eigvec]        del myHdf5[grpName+dset_eigval]    if grpName+dset_normData in myHdf5:        del myHdf5[grpName+dset_normData]    if grpName+dset_mask in myHdf5:        del myHdf5[grpName+dset_mask]    # manifold    mask_ds = myHdf5.create_dataset(grpName+dset_mask, np.shape(mask), dtype='uint8')    mask_ds[...] = mask    eigvec_ds = myHdf5.create_dataset(grpName+dset_eigvec, np.shape(Y), dtype='float64')    eigvec_ds[...] = Y    eigval_ds = myHdf5.create_dataset(grpName+dset_eigval, np.shape(s), dtype='float64')    eigval_ds[...] = s    # mask attributes    #myHdf5[grpName+dset_mask].attrs['outerR'] = args.outerR    #myHdf5[grpName+dset_mask].attrs['innerR'] = args.innerR    # diffusion map attributes    myHdf5[grpName+dset_eigvec].attrs['sigma'] = sigmaK    myHdf5[grpName+dset_eigvec].attrs['knn'] = args.knn    #myHdf5[grpName+dset_eigvec].attrs['condition'] = args.condition    myHdf5[grpName+dset_eigvec].attrs['normalize'] = args.normalize    myHdf5[grpName+dset_eigvec].attrs['denseLaplacian'] = args.denseLaplacian    #myHdf5[grpName+dset_eigvec].attrs['startRun'] = args.startRun    #myHdf5[grpName+dset_eigvec].attrs['endRun'] = args.endRun    myHdf5[grpName+dset_eigvec].attrs['runs'] = args.runs    myHdf5[grpName+dset_eigvec].attrs['numImages'] = numHits    myHdf5[grpName+dset_eigvec].attrs['exp'] = args.expName    if tag is not None: myHdf5[grpName+dset_eigvec].attrs['tag'] = tag    myHdf5[grpName+dset_eigvec].attrs['formatVersion'] = args.version    myHdf5[grpName+dset_eigvec].attrs['detectorName'] = args.detectorName    myHdf5[grpName+dset_eigvec].attrs['chunkSize'] = args.chunkSize    myHdf5[grpName+dset_eigvec].attrs['numEigs'] = args.numEigs    myHdf5[grpName+dset_eigvec].attrs['outputPath'] = args.outputPath    myHdf5[grpName+dset_eigvec].attrs['useADU'] = args.useADU    myHdf5[grpName+dset_eigvec].attrs['test'] = args.test    myHdf5[grpName+dset_eigvec].attrs['sqrt'] = args.sqrt    myHdf5[grpName+dset_eigvec].attrs['downsample'] = args.downsample    myHdf5.close()    ################################    if verbose >= 2:        print "Reading file... "        f = h5py.File(fname, 'r')        eigs = f[grpName+dset_eigvec] # manifold position        plt.plot(eigs[:,0],eigs[:,1],'x')        plt.xlabel('eig1')        plt.ylabel('eig2')        plt.title("Manifold")        plt.show()        f.close()else:    exit()MPI.Finalize()